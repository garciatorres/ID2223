{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID2223 – Lab 2:  Deep Learning with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we will use a dataset called Fashion-MNIST. The original MNIST dataset (Mixed National Institute of Standards and Technology database) is a database of handwritten digits that is commonly used for evaluating image classification algorithms. You can read more about the dataset in Yann LeCun’s MNIST page or Chris Olah’s visualizations of MNIST. MNIST is considered the HelloWorld dataset for Deep Learning, however, for this lab, MNIST is too easy to get very high accuracy on.\n",
    "\n",
    "As such, we will use Fashion-MNIST - a drop-in replacement for the original MNIST, released by Zalando. It contains images of various articles of clothing and accessories: shirts, bags, shoes, and other fashion items. The Fashion MNIST training set contains 55,000 examples, and the test set contains 10,000 examples. Each example is a 28x28 grayscale image (just like the images in the original MNIST), associated with a label from 10 classes (t-shirts, trousers, pullovers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots).\n",
    "\n",
    "This lab has the following goals:\n",
    "\n",
    "- Learn how to setup and run a computational graph in Tensorflow\n",
    "- Implement a single-layer as well as a multi-layer Neural Network in Tensorflow\n",
    "- Combine different activation functions to increase the accuracy\n",
    "- Tackle overfitting using regularization\n",
    "- Further improve the performance by using Convolutional Layers\n",
    "- Use hyperparameter optimization to improve prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create data/fashion folder, if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(\"data/fashion\"): os.makedirs(\"data/fashion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all tensorflow api is accessible through this\n",
    "import tensorflow as tf        \n",
    "# to visualize the resutls\n",
    "import matplotlib.pyplot as plt \n",
    "# 70k mnist dataset that comes with the tensorflow container\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# numpy library\n",
    "import numpy as np\n",
    "# random library\n",
    "from random import randint\n",
    "# datetime library\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Predictions with Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now many different hyperparameters in your deep neural network, from:\n",
    "- the number of layers \n",
    "- the learning rate, \n",
    "- choice of optimizer, \n",
    "- values for Dropout. \n",
    "\n",
    "In this task, you will train your network with many different combinations of hyper-parameters, with the goal of improving predictions on the test dataset.\n",
    "\n",
    "If you are running Docker/Python, you are free to use whatever methods you prefer.\n",
    "You could, for example, implement your own random search or gridsearch method or just pick a number of experiments, and use a bash script to launch different experiments with\n",
    "different combinations of hyperparameters. One framework you could use is hyperopt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_fashion_model(training_iter, ad_learning_rate, dropout, decay_rate, decay_steps):\n",
    "\n",
    "    # load data\n",
    "    mnist = input_data.read_data_sets('data/fashion', one_hot=True)\n",
    "\n",
    "    # 1. Define Variables and Placeholders\n",
    "    num_pixels = 28\n",
    "    num_inputs = num_pixels*num_pixels\n",
    "    num_outputs = 10\n",
    "    epoch_size = 100\n",
    "\n",
    "    num_hidd_1 = 200\n",
    "    num_hidd_2 = 100\n",
    "    num_hidd_3 = 60\n",
    "    num_hidd_4 = 30\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, num_pixels, num_pixels, 1], name=\"X\")\n",
    "    Y_ = tf.placeholder(tf.float32,[None, num_outputs], name=\"Y_\") # correct answers(labels)\n",
    "    XX = tf.reshape(X, [-1, num_inputs]) # flatten the images into a single line of pixels\n",
    "    keep_prob = tf.placeholder(tf.float32) #prob.of keeping a node during dropout: 1.0 at testing (no dropout) and 0.75 at training\n",
    "    global_step = tf.Variable(0, trainable=False) # decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "\n",
    "    # Weights & bias initialization\n",
    "    # One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients.\n",
    "    # Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias \n",
    "    # to avoid \"dead neurons\"\n",
    "    def weight_variable(shape):\n",
    "      initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "      initial = tf.constant(0.1, shape=shape)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    # Convolution\n",
    "    def conv2d(X, W, strides):\n",
    "      return tf.nn.conv2d(X, W, strides=strides, padding='SAME')\n",
    "\n",
    "    # 2. Define the model:\n",
    "\n",
    "    # To apply the layer, we first reshape x to a 4d tensor, with the second and third dimensions corresponding to \n",
    "    # image width and height, and the final dimension corresponding to the number of color channels.\n",
    "    x_image = tf.reshape(XX, [-1, num_pixels, num_pixels, 1])\n",
    "\n",
    "    # Conv.Layer 1: patch of 5x5, 1 input channel, 4 output channels, stride of [1,1,1,1]\n",
    "    W_conv1 = weight_variable([5, 5, 1, 4])\n",
    "    B_conv1 = bias_variable([4])\n",
    "    H_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, [1, 1, 1, 1]) + B_conv1)\n",
    "\n",
    "    # Conv.Layer 2: patch of 5x5, 4 input channel, 8 output channels, stride of [1,2,2,1]\n",
    "    W_conv2 = weight_variable([5, 5, 4, 8])\n",
    "    B_conv2 = weight_variable([8])\n",
    "    H_conv2 = tf.nn.relu(conv2d(H_conv1, W_conv2, [1, 2, 2, 1]) + B_conv2)\n",
    "    # NOTE:the image size has been reduced to 14x14 (because of the used stride)\n",
    "\n",
    "    # Conv.Layer 3: patch of 4x4, 8 input channel, 12 output channels, stride of [1,2,2,1]\n",
    "    W_conv3 = weight_variable([4, 4, 8, 12])\n",
    "    B_conv3 = weight_variable([12])\n",
    "    H_conv3 = tf.nn.relu(conv2d(H_conv2, W_conv3, [1, 2, 2, 1]) + B_conv3)\n",
    "    # NOTE:the image size has been reduced to 7x7 (because of the used stride)\n",
    "\n",
    "    # Fully connected layer (ReLU): input = vector of 12*7*7 elements, output = vector of 200 elements\n",
    "    # NOTE: You will need to reshape the tensor from having height X width X depth in matrix form \n",
    "    # to having a vector of height * width * depth elements.\n",
    "    W_fc1 = weight_variable([7 * 7 * 12, 200])\n",
    "    B_fc1 = weight_variable([200])\n",
    "    H_fc1 = tf.nn.relu(tf.matmul(tf.reshape(H_conv3,[-1, 7 * 7 * 12]),W_fc1) + B_fc1)\n",
    "    D_fc1 = tf.nn.dropout(H_fc1, keep_prob)\n",
    "\n",
    "    # Readout layer (Softmax): input = vector of 200 elements, output = vector of 10 elements\n",
    "    W_fc2 = weight_variable([200, num_outputs])\n",
    "    B_fc2 = bias_variable([num_outputs])\n",
    "\n",
    "    #YLogits: values to be used as input to softmax\n",
    "    YL = tf.matmul(D_fc1, W_fc2) + B_fc2\n",
    "\n",
    "    # 3. Define the loss function\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y_, logits=YL, name=\"loss\"))\n",
    "\n",
    "    # 4. Define the accuracy\n",
    "    correct = tf.equal(tf.argmax(YL,1), tf.argmax(Y_,1)) #tf.nn.in_top_k(Y,Y_,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    # 5.2 Train with the AdamOptimizer (a slightly better optimizer) and a starter learning rate of 0.005\n",
    "    learning_rate = tf.train.exponential_decay(ad_learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy,global_step=global_step)\n",
    "    \n",
    "    # initialize\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # training step function    \n",
    "    def training_step(i, update_test_data, update_train_data):\n",
    "        \n",
    "        # actual learning\n",
    "        batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "        sess.run(train_step, feed_dict={XX: batch_X, Y_: batch_Y, keep_prob: dropout})\n",
    "\n",
    "        tra = []\n",
    "        trc = []\n",
    "        tta = []\n",
    "        ttc = []\n",
    "\n",
    "        if update_train_data:\n",
    "            a, c = sess.run([accuracy, cross_entropy], feed_dict={XX: batch_X, Y_: batch_Y, keep_prob: dropout})\n",
    "            tra.append(a)\n",
    "            trc.append(c)\n",
    "\n",
    "        if update_test_data:\n",
    "            a, c = sess.run([accuracy, cross_entropy], feed_dict={XX: mnist.test.images, Y_: mnist.test.labels, keep_prob: dropout})\n",
    "            tta.append(a)\n",
    "            ttc.append(c)\n",
    "\n",
    "        return (tra,trc,tta,ttc)\n",
    "\n",
    "    # 6. Train and test the model, store the accuracy and loss per iteration\n",
    "    train_a = []\n",
    "    train_c = []\n",
    "    test_a = []\n",
    "    test_c = []\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        #When you compute accuracy and loss you run the whole training/testing dataset\n",
    "        #through your current model and this is quite expensive. Doing so every iteration would\n",
    "        #make it impractical, so you will compute it every 100 rounds, which we can call epochs.    \n",
    "        test = False\n",
    "        if i % epoch_size == 0: \n",
    "            test = True\n",
    "        a, c, ta, tc = training_step(i, test, test)\n",
    "        train_a += a\n",
    "        train_c += c\n",
    "        test_a += ta\n",
    "        test_c += tc\n",
    "        \n",
    "    sess.close()\n",
    "        \n",
    "    return test_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(begda, experiment, iterations, learning_r, dropout_rate, decay_rate, decay_steps, accur):\n",
    "\n",
    "    print(\"#################### Experiment No.\"+str(experiment))\n",
    "    print(\"Time: \"+str(datetime.now()-begda))\n",
    "    print(\"No.iterations:\"+str(iterations))\n",
    "    print(\"Learning rate:\"+str(learning_r))\n",
    "    print(\"Dropout rate:\"+str(dropout_rate))\n",
    "    print(\"Decay rate:\"+str(decay_rate))\n",
    "    print(\"Decay steps:\"+str(decay_steps))\n",
    "    print(\"Test accuracy:\"+str(accur[-1]))\n",
    "    print(\"Max.test accuracy:\"+str(max(accur)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.1\n",
      "Time: 0:08:27.853597\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.96\n",
      "Decay steps:100\n",
      "Test accuracy:0.7905\n",
      "Max.test accuracy:0.7993\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.2\n",
      "Time: 0:08:31.228494\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.9\n",
      "Decay steps:100\n",
      "Test accuracy:0.7419\n",
      "Max.test accuracy:0.7476\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.3\n",
      "Time: 0:08:45.562100\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.96\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8229\n",
      "Max.test accuracy:0.8229\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.4\n",
      "Time: 0:08:58.625777\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.9\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8082\n",
      "Max.test accuracy:0.8089\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.5\n",
      "Time: 0:09:24.175734\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.96\n",
      "Decay steps:100\n",
      "Test accuracy:0.8579\n",
      "Max.test accuracy:0.8607\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.6\n",
      "Time: 0:09:22.427823\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.9\n",
      "Decay steps:100\n",
      "Test accuracy:0.8212\n",
      "Max.test accuracy:0.8281\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.7\n",
      "Time: 0:09:24.647898\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.96\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8817\n",
      "Max.test accuracy:0.8832\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.8\n",
      "Time: 0:09:19.983080\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.9\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8719\n",
      "Max.test accuracy:0.8767\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.9\n",
      "Time: 0:09:20.466392\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.96\n",
      "Decay steps:100\n",
      "Test accuracy:0.8768\n",
      "Max.test accuracy:0.8814\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.10\n",
      "Time: 0:09:19.664940\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.9\n",
      "Decay steps:100\n",
      "Test accuracy:0.8581\n",
      "Max.test accuracy:0.863\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.11\n",
      "Time: 0:09:20.811253\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.96\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8934\n",
      "Max.test accuracy:0.895\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.12\n",
      "Time: 0:09:23.088551\n",
      "No.iterations:10000\n",
      "Learning rate:0.001\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.9\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8902\n",
      "Max.test accuracy:0.8926\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.13\n",
      "Time: 0:09:24.581425\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.96\n",
      "Decay steps:100\n",
      "Test accuracy:0.8219\n",
      "Max.test accuracy:0.8263\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.14\n",
      "Time: 0:09:25.477988\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.9\n",
      "Decay steps:100\n",
      "Test accuracy:0.8045\n",
      "Max.test accuracy:0.8121\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.15\n",
      "Time: 0:09:22.206234\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.96\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8204\n",
      "Max.test accuracy:0.8248\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.16\n",
      "Time: 0:09:23.017661\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.9\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8207\n",
      "Max.test accuracy:0.8314\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.17\n",
      "Time: 0:09:31.110555\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.96\n",
      "Decay steps:100\n",
      "Test accuracy:0.8842\n",
      "Max.test accuracy:0.8897\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.18\n",
      "Time: 0:09:37.173476\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.9\n",
      "Decay steps:100\n",
      "Test accuracy:0.8697\n",
      "Max.test accuracy:0.8743\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.19\n",
      "Time: 0:09:34.519724\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.96\n",
      "Decay steps:1000\n",
      "Test accuracy:0.87\n",
      "Max.test accuracy:0.8749\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.20\n",
      "Time: 0:09:36.162754\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.9\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8719\n",
      "Max.test accuracy:0.8758\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.21\n",
      "Time: 0:09:33.289450\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.96\n",
      "Decay steps:100\n",
      "Test accuracy:0.8891\n",
      "Max.test accuracy:0.8941\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.22\n",
      "Time: 0:09:33.316185\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.9\n",
      "Decay steps:100\n",
      "Test accuracy:0.8867\n",
      "Max.test accuracy:0.8905\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.23\n",
      "Time: 0:09:37.518976\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.96\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8896\n",
      "Max.test accuracy:0.8936\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.24\n",
      "Time: 0:09:32.768521\n",
      "No.iterations:10000\n",
      "Learning rate:0.005\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.9\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8937\n",
      "Max.test accuracy:0.898\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.25\n",
      "Time: 0:09:35.844166\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.96\n",
      "Decay steps:100\n",
      "Test accuracy:0.7867\n",
      "Max.test accuracy:0.7953\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.26\n",
      "Time: 0:09:36.195275\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.9\n",
      "Decay steps:100\n",
      "Test accuracy:0.7535\n",
      "Max.test accuracy:0.7675\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.27\n",
      "Time: 0:09:35.674369\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.96\n",
      "Decay steps:1000\n",
      "Test accuracy:0.78\n",
      "Max.test accuracy:0.7822\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.28\n",
      "Time: 0:09:38.538479\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.1\n",
      "Decay rate:0.9\n",
      "Decay steps:1000\n",
      "Test accuracy:0.7856\n",
      "Max.test accuracy:0.7957\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.29\n",
      "Time: 0:09:38.019914\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.96\n",
      "Decay steps:100\n",
      "Test accuracy:0.8701\n",
      "Max.test accuracy:0.8742\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.30\n",
      "Time: 0:09:01.695863\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.9\n",
      "Decay steps:100\n",
      "Test accuracy:0.8689\n",
      "Max.test accuracy:0.8716\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.31\n",
      "Time: 0:10:09.795755\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.96\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8466\n",
      "Max.test accuracy:0.8499\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.32\n",
      "Time: 0:12:09.687190\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.25\n",
      "Decay rate:0.9\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8583\n",
      "Max.test accuracy:0.8629\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.33\n",
      "Time: 0:12:06.837449\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.96\n",
      "Decay steps:100\n",
      "Test accuracy:0.8973\n",
      "Max.test accuracy:0.8973\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.34\n",
      "Time: 0:10:03.541117\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.9\n",
      "Decay steps:100\n",
      "Test accuracy:0.8872\n",
      "Max.test accuracy:0.8915\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.35\n",
      "Time: 0:09:27.851172\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.96\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8702\n",
      "Max.test accuracy:0.872\n",
      "\n",
      "\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "#################### Experiment No.36\n",
      "Time: 0:09:33.427499\n",
      "No.iterations:10000\n",
      "Learning rate:0.01\n",
      "Dropout rate:0.5\n",
      "Decay rate:0.9\n",
      "Decay steps:1000\n",
      "Test accuracy:0.8836\n",
      "Max.test accuracy:0.8836\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization experiment\n",
    "num_iterations = [10000]\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "dropout_rates = [0.1, 0.25, 0.5]\n",
    "decay_steps = [100, 1000]\n",
    "decay_rates = [0.96, 0.90]\n",
    "experiment = 0\n",
    "\n",
    "for i in num_iterations:\n",
    "    for lr in learning_rates:\n",
    "        for kr in dropout_rates:\n",
    "            for ds in decay_steps:\n",
    "                for dr in decay_rates:\n",
    "                    experiment +=1\n",
    "                    begda = datetime.now()\n",
    "                    accur = mnist_fashion_model(i, lr, kr, dr, ds)\n",
    "                    results(begda, experiment, i, lr, kr, dr, ds, accur)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
